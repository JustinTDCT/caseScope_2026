# OpenSearch Shard Limit Crisis - Complete Fix

**Date**: 2025-11-06  
**Issue**: Worker crash when processing multiple files simultaneously  
**Root Cause**: OpenSearch cluster hit maximum shard limit (10,000/10,000)  
**Impact**: All file indexing operations failed immediately with no recovery

---

## ‚ö†Ô∏è Problem Summary

### What Happened

User was indexing 2 cases simultaneously when the Celery worker suddenly died. All subsequent file processing attempts failed immediately with the error:

```
RequestError(400, 'validation_exception', 'Validation Failed: 1: this action would add [1] total shards, but this cluster currently has [10000]/[10000] maximum shards open;')
```

### Why This Happened

1. **Each uploaded EVTX file creates a new OpenSearch index**
2. **Each index = 1 shard** (with 1 primary, 0 replicas by design)
3. **Default OpenSearch limit = 10,000 shards per node**
4. **With ~10,000 files indexed**, the system reached the hard limit
5. **2 cases indexing at once** pushed the system over the edge simultaneously

### Critical Impact

- ‚úó All file processing stopped completely
- ‚úó Worker continued accepting tasks but all failed
- ‚úó No graceful degradation or warning
- ‚úó Generic error messages didn't reveal root cause
- ‚úó System appeared "working" but was completely broken

---

## üîß Immediate Fix (Applied)

### 1. Increased OpenSearch Shard Limit

**Previous**: 10,000 shards per node (default)  
**New**: 50,000 shards per node  

```bash
curl -X PUT "http://localhost:9200/_cluster/settings" -H 'Content-Type: application/json' -d'
{
  "persistent": {
    "cluster.max_shards_per_node": "50000"
  }
}'
```

**Verification**:
```bash
curl -s http://localhost:9200/_cluster/settings | grep max_shards_per_node
# Output: "max_shards_per_node":"50000"
```

---

## üõ°Ô∏è Long-Term Protection (Implemented)

### 2. Pre-Flight Shard Capacity Check

**Added to**: `/opt/casescope/app/tasks.py`

**New Function**: `check_opensearch_shard_capacity()`

```python
def check_opensearch_shard_capacity(opensearch_client, threshold_percent=90):
    """
    Check if OpenSearch cluster has capacity for more shards
    Returns: (has_capacity: bool, current_shards: int, max_shards: int, message: str)
    """
    # Gets current shard count from cluster
    # Compares against max_shards_per_node * node_count
    # Returns False if > threshold_percent full
```

**Integration in `process_file()` task**:

```python
# CRITICAL: Check OpenSearch shard capacity before processing
# This prevents the worker from crashing when hitting shard limits
if operation in ['full', 'reindex']:
    has_capacity, current_shards, max_shards, shard_message = check_opensearch_shard_capacity(
        opensearch_client, threshold_percent=95
    )
    logger.info(f"[TASK] {shard_message}")
    
    if not has_capacity:
        error_msg = f"OpenSearch shard limit nearly reached ({current_shards:,}/{max_shards:,}). Please consolidate indices or increase shard limit."
        logger.error(f"[TASK] {error_msg}")
        case_file.indexing_status = 'Failed'
        case_file.error_message = error_msg
        db.session.commit()
        return {'status': 'error', 'message': error_msg, ...}
```

**Benefits**:
- ‚úÖ Checks capacity **before** attempting to create index
- ‚úÖ Prevents worker crashes from hard shard limit errors
- ‚úÖ Provides clear error messages to users
- ‚úÖ Gracefully fails with actionable guidance
- ‚úÖ Logs warnings at 95% capacity threshold

### 3. Enhanced Error Detection

**Modified**: `/opt/casescope/app/file_processing.py`

**Better Shard Limit Error Handling**:

```python
except Exception as e:
    logger.error(f"[INDEX FILE] Failed to create index {index_name}: {e}")
    
    # Check if this is a shard limit error
    error_str = str(e)
    if 'maximum shards open' in error_str or 'max_shards_per_node' in error_str:
        logger.critical(f"[INDEX FILE] ‚ö†Ô∏è  OPENSEARCH SHARD LIMIT REACHED - Cannot create more indices")
        case_file.indexing_status = 'Failed: Shard Limit'
        case_file.error_message = 'OpenSearch shard limit reached. Please consolidate indices or increase cluster.max_shards_per_node setting.'
    else:
        # Generic index creation failure
        case_file.indexing_status = f'Failed: {str(e)[:100]}'
        case_file.error_message = str(e)[:500]
```

**Benefits**:
- ‚úÖ Specific error status: `'Failed: Shard Limit'`
- ‚úÖ Critical log entry for alerting
- ‚úÖ Actionable guidance in error message
- ‚úÖ Distinguishes shard limit errors from other failures

### 4. Error Message Tracking

**Database Schema Change**: Added `error_message` column to `case_file` table

```sql
ALTER TABLE case_file ADD COLUMN error_message TEXT;
```

**Model Update**: `/opt/casescope/app/models.py`

```python
class CaseFile(db.Model):
    ...
    indexing_status = db.Column(db.String(50), default='Queued')
    error_message = db.Column(db.Text)  # NEW: Detailed error tracking
    ...
```

**Benefits**:
- ‚úÖ Stores full error details (up to 500 chars)
- ‚úÖ Enables better debugging of failed files
- ‚úÖ Users can see why specific files failed
- ‚úÖ Supports future UI improvements (error tooltips, retry logic)

---

## üìä Current System State

**Cluster Status**:
- **Indices**: 9,999 (1 less than we started, index cleanup may have occurred)
- **Shards**: 10,000 primaries + 0 replicas = 10,000 total
- **Limit**: 50,000 shards per node
- **Capacity**: 20% utilized (10,000 / 50,000)
- **Headroom**: 40,000 additional shards available

**Current Capacity**:
- Can add ~40,000 more files before hitting new limit
- At previous rate (10,000 files), that's 4x current capacity
- Should provide adequate runway for near-term growth

---

## üéØ Prevention Strategy Going Forward

### Immediate Monitoring (Manual)

**Check Shard Usage**:
```bash
curl -s http://localhost:9200/_cluster/stats | python3 -m json.tool | grep -A 3 '"shards"'
```

**Check Shard Limit**:
```bash
curl -s http://localhost:9200/_cluster/settings | grep max_shards_per_node
```

**Calculate Capacity**:
```python
# In Python console or script
current_shards / max_shards * 100  # Should be < 95%
```

### Automatic Protection (Implemented)

1. **Pre-flight capacity check** (95% threshold)
   - Blocks new indexing operations when near limit
   - Provides clear error messages
   - Prevents worker crashes

2. **Enhanced error detection**
   - Identifies shard limit errors specifically
   - Logs at CRITICAL level for alerting
   - Stores detailed error messages

3. **Graceful degradation**
   - Files marked as 'Failed: Shard Limit' (not generic failure)
   - Worker continues processing other operations
   - No crash/restart cycle

### Future Improvements (Recommended)

#### Short-Term (1-2 weeks)

1. **Dashboard Widget**: Display current shard usage
   ```
   OpenSearch Shards: 10,234 / 50,000 (20.5%)
   Status: Healthy ‚úì
   ```

2. **Alert Thresholds**:
   - 80% = Warning (40,000 shards)
   - 90% = High (45,000 shards)
   - 95% = Critical (47,500 shards)

3. **Email Notifications**: Alert admins at threshold breaches

#### Medium-Term (1-2 months)

1. **Index Consolidation Script**:
   - Merge indices for closed cases
   - Archive old/unused indices
   - Implement index lifecycle management

2. **Shard Optimization**:
   - Evaluate if one-index-per-file is necessary
   - Consider daily/weekly rollover indices
   - Group small files into shared indices

3. **Auto-Scaling**:
   - Automatically increase shard limit when approaching threshold
   - Add OpenSearch cluster nodes if needed

#### Long-Term (3-6 months)

1. **Index Archival Strategy**:
   - Move indices for cases older than X months to cold storage
   - Implement snapshot/restore for archived cases
   - Tiered storage (hot/warm/cold)

2. **Alternative Indexing Strategies**:
   - Parent-child relationships (case ‚Üí files)
   - Nested documents (case contains file events)
   - Time-based indices (case_X_YYYY-MM-DD)

3. **Capacity Planning Dashboard**:
   - Trend analysis of shard growth
   - Projected time until limit
   - Recommended maintenance actions

---

## üîç Root Cause Analysis

### Why Didn't We See This Coming?

1. **No monitoring**: Shard count wasn't tracked
2. **No warning system**: Hit hard limit with no degradation
3. **Generic errors**: Worker logs didn't mention shards
4. **Concurrent processing**: 2 cases racing to create indices

### Why Did It Crash So Badly?

1. **Synchronous failures**: Every new file tried to create index
2. **No retry logic**: Worker accepted task ‚Üí tried to index ‚Üí failed immediately
3. **Hidden from UI**: Files showed "Queued" but were actually failing
4. **Silent failure**: Worker didn't crash, just failed every task

### Why Didn't Pre-flight Checks Exist?

1. **Happy path focus**: Code assumed index creation would succeed
2. **OpenSearch abstraction**: Developer didn't consider cluster limits
3. **Scale blindness**: Worked fine with 100 files, 1,000 files, then suddenly failed at 10,000

---

## ‚úÖ Validation & Testing

### Immediate Verification (Completed)

1. ‚úÖ Increased shard limit to 50,000
2. ‚úÖ Added pre-flight capacity checks
3. ‚úÖ Enhanced error detection and logging
4. ‚úÖ Added error_message column to database
5. ‚úÖ Fixed log file permissions (casescope:casescope)
6. ‚úÖ Restarted worker and web services
7. ‚úÖ Verified no linting errors
8. ‚úÖ Cleared Python cache

### Service Status

```bash
‚óè casescope.service - CaseScope 2026 Web Application
   Active: active (running)
   
‚óè casescope-worker.service - CaseScope 2026 Celery Worker
   Active: active (running)
   Concurrency: 4 workers
```

### Next Steps for User

1. **Retry Failed Files**:
   - Check case file dashboard for files with status "Failed: Shard Limit"
   - Use "Reindex" button to retry those files
   - They should now process successfully

2. **Monitor Shard Usage** (manual for now):
   ```bash
   curl -s http://localhost:9200/_cluster/stats | grep -A 3 '"shards"'
   ```

3. **Review Log Files**:
   - `/opt/casescope/logs/workers.log` - Should show pre-flight capacity checks
   - Look for: `[TASK] OpenSearch Shards: X/50,000 (Y%)`

---

## üìù Files Modified

1. `/opt/casescope/app/tasks.py`
   - Added `check_opensearch_shard_capacity()` function
   - Integrated pre-flight shard check in `process_file()` task

2. `/opt/casescope/app/file_processing.py`
   - Enhanced error detection for shard limit errors
   - Added specific error status and messages

3. `/opt/casescope/app/models.py`
   - Added `error_message` column to `CaseFile` model

4. **Database** (PostgreSQL)
   - Added `error_message TEXT` column to `case_file` table

5. **OpenSearch Cluster Settings**
   - Increased `cluster.max_shards_per_node` from 10,000 to 50,000

---

## üéì Lessons Learned

1. **Monitor resource limits**, not just resource usage
2. **Implement pre-flight checks** for operations with hard limits
3. **Differentiate error types** (transient vs. permanent vs. resource exhaustion)
4. **Scale testing matters** - 10x isn't always 100x
5. **Concurrent operations** can race to exhaust resources faster than expected
6. **Graceful degradation** is better than silent failure
7. **Actionable error messages** save hours of debugging

---

## üîí Deployment Complete

**Status**: ‚úÖ **RESOLVED**

**Immediate Fixes Applied**:
- OpenSearch shard limit increased (5x capacity)
- Pre-flight capacity checks implemented
- Enhanced error detection and logging
- Error message tracking in database
- Log file permissions corrected
- Services restarted and verified

**System Health**: ‚úÖ **OPERATIONAL**

**User Action Required**: Retry files that failed with "Shard Limit" errors

---

**Document Version**: 1.0  
**Last Updated**: 2025-11-06 23:15 UTC  
**Author**: CaseScope AI Assistant  
**Reviewed**: Pending

